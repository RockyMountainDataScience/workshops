{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 9 - Neural Networks\n",
    "\n",
    "In this workshop, we will learn about [neural networks](https://en.wikipedia.org/wiki/Artificial_neural_network), what they are and how to implement one. This is just an introduction to a very in depth subject. For more introduction, you might check our some of these [youtube videos](https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw) by 3Blue1Brown, or this [free book](https://web.stanford.edu/~hastie/ElemStatLearn/) by Hastie and Tibshirani, or this non-free (but reasonably priced)  [book](https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618/ref=pd_bxgy_img_3/133-1872783-0797223?_encoding=UTF8&pd_rd_i=0262035618&pd_rd_r=5d83c17f-22d1-4b69-abce-1d42787589e6&pd_rd_w=XdkjL&pd_rd_wg=bKmxG&pf_rd_p=09627863-9889-4290-b90a-5e9f86682449&pf_rd_r=JYV6WZ2R4GBAR2YTTQG9&psc=1&refRID=JYV6WZ2R4GBAR2YTTQG9) by Goodfellow, Bengio and Courville.\n",
    "\n",
    "Although there is a great deal of hype surrounding neural networks which make them seem magical and mysterious, we will make clear that they are simply nonlinear statistical models. Before we do, let's briefly discuss linear regression.\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "Let's consider the simple linear regression model to model the **mean** of a response $y$ as simple linear function of single quantitative variable $x$. \n",
    "\n",
    "$$ y = mx + b $$\n",
    "\n",
    "Further, suppose that there is some amount of variability in a given observation about this mean. We can incorporate this into our model by adding an error term $\\epsilon$ (centered about zero) giving the model the following form:\n",
    "\n",
    "$$ y = mx + b + \\epsilon $$\n",
    "\n",
    "where, for example, we might have $\\epsilon \\sim N(0,1)$ (i.e. normally distributed errors with variance equal to one).\n",
    "\n",
    "The estimated model will then have the form:\n",
    "\n",
    "$$ \\hat y = \\hat m x + \\hat b $$\n",
    "\n",
    "Let's look at a concrete example. Consider the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import sklearn\n",
    "\n",
    "\n",
    "n = 100                  # Sample size equals 100\n",
    "eps = np.random.randn(n) # Errors ~ N(0, 1)\n",
    "m = 2                    # Slope equal to 2\n",
    "x = np.random.random(n)  # x values uniformly distributed across [0,1]\n",
    "b = 3                    # Y intercept equal to 3\n",
    "\n",
    "y = m * x + b + eps # Generate the y data\n",
    "\n",
    "# Estimate the model with linregress\n",
    "(a_s, b_s, r, tt, stderr) = stats.linregress(x, y)\n",
    "\n",
    "# Print estimated coefficients\n",
    "print(a_s) # m\n",
    "print(b_s) # b\n",
    "\n",
    "# Setup predicted x and y values to plot\n",
    "x_pred = np.linspace(0,1,100)\n",
    "y_pred = a_s*x_pred+b_s\n",
    "\n",
    "# We can estimate this model with \n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x_pred, y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters (m and b) in this model are estimated via least squares, meaning that the sum of the squared errors (the vertical distance between an observation and the estimated mean) are minimized. If this indeed is the true model, least squares in the minimum variance unbiased estimator (UMVUE) of the true mean. But, if we misspecify the model, we might have large differences between the true model and our estimated model. For instance, consider the data being generated from a different model (a quadratic model in this case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100 # Sample size\n",
    "eps = np.random.randn(n) # Errors ~ N(0, 1)\n",
    "m = 2\n",
    "l = 3\n",
    "x = 5 *np.random.random(n) # x values uniformly distributed across [0,1]\n",
    "b = 3\n",
    "y = m * x + l * x**2 + b + eps\n",
    "\n",
    "(a_s, b_s, r, tt, stderr) = stats.linregress(x, y)\n",
    "\n",
    "print(a_s) # m\n",
    "print(b_s) # b\n",
    "\n",
    "x_pred = np.linspace(0,5,100)\n",
    "y_pred = a_s*x_pred+b_s\n",
    "\n",
    "# We can estimate this model with \n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x_pred, y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, when we fit a linear regression model, it is important that we get an adequate representation of the functional form of the model. We could still fit this data with a linear regression model. For example we could estimate the model:\n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\epsilon $$\n",
    "\n",
    "and obtain an unbiased estimate of the true mean. This is second order univariate polynomial regression model and is still a linear model (it is **linear in the model terms**, i.e. it is linear in it's functional form).\n",
    "\n",
    "A linear regression model may be a function of multiple variables. In general a simple linear regression model with $p$ variables might have the form:\n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p + \\epsilon $$\n",
    "\n",
    "Instead of the one-dimensional fitted line as in the preceding examples, the estimated mean will be a p-dimensional surface. However, note that our assumption of linearity might be somewhat restricting, as we can only estimate models that can be approximated by polynomials. This is where neural networks will have an advantage, as they are a class of statistical models with a great deal of flexibility in their functional form.\n",
    "\n",
    "## Neural Networks\n",
    "\n",
    "A neural network can be considered as a nonparametric and nonlinear estimator of a function. It will have a great deal more flexibility in form than our linear regression model we have seen previously. \n",
    "\n",
    "### A nonlinear data generating process\n",
    "\n",
    "Let us consider the following true data generating process:\n",
    "\n",
    "$$ y = \\theta_1 - \\theta_2 exp( - exp(\\theta_3 + \\theta_4 log(x))) + \\epsilon$$\n",
    "\n",
    "With $\\theta_1 = 80$, $\\theta_2 = 71$, $\\theta_3 = -10$, $\\theta_4 = 2.4$ and $\\epsilon \\sim N(0, \\sigma^2)$ and $\\sigma = 2$. This is an example of a Weibull model used in [this book](https://link.springer.com/book/10.1007%2Fb97288) to model pasture regrowth. If we were equiped with this functional form and interested specifically in the values of these parameters, we could estimate these parameters via nonlinear least squares in a similar fashion of our previous linear regression model. For instance, estimating these parameters may have specific biological meaning to this pasture regrowth system. However, if we are merely interested in a model that predicts the mean well, we merely need a function that fits the data well. A neural network model will predict the mean well, but will not have a functional form amenable to such interpretation. \n",
    "\n",
    "Let's take a look at the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nonlinear():\n",
    "    \"\"\"\n",
    "    A Nonlinear Model Class: \n",
    "    params: theta_1 - theta_4\n",
    "    param: sigma - Standard deviation\n",
    "    param: n - sample size\n",
    "    param: x_min, x_max\n",
    "    function: simulate function\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n=40, sigma=2, theta_1=80, theta_2=71, theta_3=-10, theta_4=2.4, x_min=0.1, x_max=90):\n",
    "        self.n = n\n",
    "        self.sigma = sigma\n",
    "        self.theta_1 = theta_1\n",
    "        self.theta_2 = theta_2\n",
    "        self.theta_3 = theta_3\n",
    "        self.theta_4 = theta_4\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "        #self.df = self.simulate()\n",
    "\n",
    "    def simulate(self, sample_size = 40):\n",
    "        self.x = sorted(np.random.uniform(self.x_min, self.x_max, sample_size))\n",
    "        self.y = self.theta_1 - self.theta_2 * np.exp(\n",
    "            -np.exp(self.theta_3 + self.theta_4 * np.log(self.x))) + np.random.normal(0, self.sigma, sample_size)\n",
    "\n",
    "    def plot(self):\n",
    "        lin_x = np.linspace(self.x_min, self.x_max, 100)\n",
    "        lin_y = self.theta_1 - self.theta_2 * np.exp(-np.exp(self.theta_3 + self.theta_4 * np.log(lin_x)))\n",
    "        plt.scatter(self.x, self.y)\n",
    "        plt.plot(lin_x, lin_y, color = 'red')\n",
    "        plt.show()\n",
    "        \n",
    "nonlinear_model = nonlinear()\n",
    "nonlinear_model.simulate()\n",
    "nonlinear_model.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. Now that we are equiped with a nonlinear data generating process, we want to estimate it with a neural network. So what exactly is a neural network?\n",
    "\n",
    "### Neural Networks\n",
    "\n",
    "A neural network models a response as a linear combination nonlinear 'activation' functions. Specifically, a simple single layer neural network has the form:\n",
    "\n",
    "\\begin{align*}\n",
    "y &= \\beta_0 + \\sum_{i=1}^{k} \\beta_k * Z_m \\\\\n",
    "&= \\beta_0 + \\sum_{i=1}^{k} \\beta_k * \\sigma( \\alpha_{0p} + \\sum_{j = 1}^p \\alpha_j x_p) \\\\\n",
    "\\end{align*} \n",
    "\n",
    "where $\\beta_0, \\dots \\beta_k$ and $\\alpha_0, \\dots, \\alpha_j$ are model parameters and $\\sigma$ is an 'activation' function.\n",
    "\n",
    "If we stare at this function long enough, we will realize that indeed a neural network is simply a nonlinear model that is a linear combination of nonlinear functions of linear combinations (the sum parts of the model). \n",
    "\n",
    "The activation function has a special task in the above equation and it is to for the 'neuron' to 'activate' when the linear combination results in a value large or small enough. Let's take a look at the hyperbolic tangent activation function:\n",
    "\n",
    "![image](./images/activation_ex.png)\n",
    "\n",
    "We see that the value will gets close to -1 or 1 as the input gets small or large enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. So that's the functional form of a neural network. Where is the network you might ask? It is simply a way to visualize the functional form of this model.\n",
    "\n",
    "![image](./images/nnet_ex.png)\n",
    "\n",
    "Ok. So now that we understand what a neural network is a bit more, let's build one with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neural_net(simulated_data):\n",
    "  \"\"\"\n",
    "  Function to run fitting a keras model\n",
    "  \"\"\"\n",
    "\n",
    "  # Alias simulated data\n",
    "  X = simulated_data.x\n",
    "  y = simulated_data.y\n",
    "  # Print Training Time\n",
    "  clf = MLPRegressor(\n",
    "        activation='logistic', alpha=0, batch_size='auto', beta_1=0.9,\n",
    "        beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "        hidden_layer_sizes=(100,), learning_rate='invscaling',\n",
    "        learning_rate_init=0.001, max_iter=10000, momentum=0.99,\n",
    "        nesterovs_momentum=True, power_t=0.5,  random_state=1,\n",
    "        shuffle=True, solver='lbfgs', tol=0.001, validation_fraction=0.0,\n",
    "        verbose=False, warm_start=True\n",
    "    )\n",
    "  return clf\n",
    "X = nonlinear_model.x\n",
    "y = nonlinear_model.y\n",
    "clf = build_neural_net(nonlinear_model)\n",
    "clf.fit(np.array(X).reshape((40,1)), y)\n",
    "x_pred = np.arange(0,90, .01).reshape((9000, 1))\n",
    "preds = clf.predict(x_pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.plot(x_pred, preds, color = 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that although we model fits the data fairly well, it may not generalize well to unseen data. As we did in the workshop where we used tree depth to control smoothing and cross validation to choose maximum tree depth, we can do a similar process with neural networks. However, in this case, we will use L2 regularizatioon, which controls the size of the coefficient vector. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_neural_net(simulated_data):\n",
    "  \"\"\"\n",
    "  Function to run fitting a keras model\n",
    "  \"\"\"\n",
    "\n",
    "  # Alias simulated data\n",
    "  X = simulated_data.x\n",
    "  y = simulated_data.y\n",
    "  # Print Training Time\n",
    "  clf = MLPRegressor(\n",
    "    activation='logistic', alpha=0.4, batch_size='auto', beta_1=0.9,\n",
    "    beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "    hidden_layer_sizes=(100,), learning_rate='invscaling',\n",
    "    learning_rate_init=0.001, max_iter=10000, momentum=0.99,\n",
    "    nesterovs_momentum=True, power_t=0.5,  random_state=1,\n",
    "    shuffle=True, solver='lbfgs', tol=0.001, validation_fraction=0.0,\n",
    "    verbose=False, warm_start=True\n",
    "    )\n",
    "  return clf\n",
    "X = nonlinear_model.x\n",
    "y = nonlinear_model.y\n",
    "clf = build_neural_net(nonlinear_model)\n",
    "clf.fit(np.array(X).reshape((40,1)), y)\n",
    "x_pred = np.arange(0,90, .01).reshape((9000, 1))\n",
    "preds = clf.predict(x_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.plot(x_pred, preds, color = 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the function is much smoother and closer to the ground truth. We won't go through a rigorous tuning of this model, which would involve using cross-validation for the optimal choice of a large number of tunable parameters. Instead, we will briefly describe some of these tuning parameters and let you all play around with the parameters of a neural network to understand what these tunable parameters will do for our model. \n",
    "\n",
    "First, there are a number of choices relating to parameters of the algorithm used to estimate the model. Rather than being able to solve for coefficients via least squares as in the linear regression case, we will estimate the model with  another process that won't necessarily return the optimal solution. These parameters control convergence of a given algorithm. For instance, in the above example, the 'lfbgs' method was used to estimate the model, and the following parameters are for the algorithm used:\n",
    "\n",
    "- batch_size\n",
    "- beta_1\n",
    "- beta_2\n",
    "- early_stopping\n",
    "- epsilon\n",
    "- learning_rate\n",
    "- learning_rate_init\n",
    "- nesterovs_momentum\n",
    "- power_t\n",
    "- shuffle\n",
    "- solver\n",
    "- tol\n",
    "- validation_fraction\n",
    "- warm_start\n",
    "\n",
    "Indeed, most of the parameters being used relate to the estimation process. Other's control the functional form of the model:\n",
    "\n",
    "- activation\n",
    "- hidden_layer_size\n",
    "- alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity\n",
    "\n",
    "In the following activity, we will play with the the parameters of the nueral network classifier to classify the make_moons and make_circles and linearly seperable datasets. Look at the documentation for the [MLPClassifier function](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) to choose and play around with the parameters of the neural network to get an idea on the effect of different parameters. Once you're happy with your tuned neural network, continue this exploration by heading over to this [neural network playground](https://playground.tensorflow.org/) from tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "names = [\"Neural Net\"]\n",
    "\n",
    "classifiers = [\n",
    " \n",
    "    MLPClassifier(\n",
    "        activation='logistic', alpha=0, batch_size='auto', beta_1=0.9,\n",
    "        beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "        hidden_layer_sizes=(1,), learning_rate='invscaling',\n",
    "        learning_rate_init=0.001, max_iter=100, momentum=0.99,\n",
    "        nesterovs_momentum=True, power_t=0.5,  random_state=1,\n",
    "        shuffle=True, solver='lbfgs', tol=0.001, validation_fraction=0.0,\n",
    "        verbose=False, warm_start=True\n",
    "                 )]\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [make_moons(n_samples = 200, noise=0.3, random_state=0),\n",
    "            make_circles(n_samples = 200, noise=0.2, factor=0.5, random_state=1),\n",
    "            linearly_separable\n",
    "            ]\n",
    "\n",
    "figure = plt.figure(figsize=(9, 9))\n",
    "i = 1\n",
    "# iterate over datasets\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "               edgecolors='k')\n",
    "    # Plot the testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n",
    "               edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "        # Plot the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "                   edgecolors='k')\n",
    "        # Plot the testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "                   edgecolors='k', alpha=0.6)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(name)\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                size=15, horizontalalignment='right')\n",
    "        i += 1\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
