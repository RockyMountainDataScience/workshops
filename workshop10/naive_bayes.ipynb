{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Spam Filter\n",
    "In this notebook we will train and validate a Naive Bayes spam filter (classifier). The plan is to use the [UCI spambase dataset](https://archive.ics.uci.edu/ml/datasets/Spambase) to determine what messages are spam or not spam. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem\n",
    "\n",
    "Given the event that the message is spam by S and the message contains a word by W, Bayes theorem allows us to calculate the probability that the message is spam given the word(s) from the trained probabilities of the message is spam if it contains a word. \n",
    "\n",
    "$$ \n",
    "P(S|W) = \\frac{P(W|S)P(S)}{P(W)} = \\frac{P(W|S)P(S)}{P(W|S)P(S) + P(W|!S)P(!S)} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import sklearn.metrics\n",
    "\n",
    "import get_words\n",
    "\n",
    "np.random.seed(123) # So you can exactly reproduce my numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_catagories = np.array(get_words.get_words())\n",
    "\n",
    "def load_data():\n",
    "    \"\"\" \n",
    "    Load the spambase dataset\n",
    "    \"\"\"\n",
    "    column_names = np.concatenate((word_catagories, ['target']))\n",
    "    column_numbers = np.concatenate((np.arange(len(word_catagories)), [57]))\n",
    "\n",
    "    data = pd.read_csv(os.path.join('.', 'data', 'spambase.data'), \n",
    "                       names=column_names, usecols=column_numbers)\n",
    "    #data[word_catagories] *= 10000 # Scaling factor to go from percentage to number of words \n",
    "    return data\n",
    "\n",
    "def split_data(data, p):\n",
    "    \"\"\"\n",
    "    Split the data according to the probability p of picking a training sample.\n",
    "    Returns data with the same number of columns, except the first DataFrame \n",
    "    (training set) has p*data.shape[0] number of rows and the second DataFrame \n",
    "    (validation set) has (1-p)*data.shape[0] number of rows.\n",
    "    \"\"\" \n",
    "    n_samples = data.shape[0]\n",
    "    n_train = int(n_samples * p)\n",
    "    idt = np.random.choice(np.arange(n_samples), size=n_train, replace=False)\n",
    "    idv = np.array([i for i in range(n_samples) if i not in idt])\n",
    "    return data.loc[idt], data.loc[idv]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the spambase dataset, and split it into a train and validate subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()\n",
    "train, validate = split_data(data, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>make</th>\n",
       "      <th>address</th>\n",
       "      <th>all</th>\n",
       "      <th>our</th>\n",
       "      <th>over</th>\n",
       "      <th>remove</th>\n",
       "      <th>internet</th>\n",
       "      <th>order</th>\n",
       "      <th>mail</th>\n",
       "      <th>receive</th>\n",
       "      <th>...</th>\n",
       "      <th>direct</th>\n",
       "      <th>cs</th>\n",
       "      <th>meeting</th>\n",
       "      <th>original</th>\n",
       "      <th>project</th>\n",
       "      <th>re</th>\n",
       "      <th>edu</th>\n",
       "      <th>table</th>\n",
       "      <th>conference</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3938</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3533</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3110</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2709</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      make  address  all  our  over  remove  internet  order  mail  receive  \\\n",
       "3938   0.0      0.0  0.0  0.0  0.00     0.0       0.0    0.0   0.0     0.45   \n",
       "3533   0.9      0.0  0.0  0.0  0.00     0.0       0.0    0.0   0.0     0.00   \n",
       "3110   0.0      0.0  0.0  0.0  0.00     0.0       0.0    0.0   0.0     0.00   \n",
       "892    0.0      0.0  0.0  0.0  0.73     0.0       0.0    0.0   0.0     0.00   \n",
       "2709   0.0      0.0  0.0  0.0  0.00     0.0       0.0    0.0   0.0     0.28   \n",
       "\n",
       "      ...  direct   cs  meeting  original  project   re  edu  table  \\\n",
       "3938  ...     0.0  0.0     0.00       0.0     0.00  0.0  0.0    0.0   \n",
       "3533  ...     0.0  0.0     0.00       0.0     0.00  0.0  0.0    0.0   \n",
       "3110  ...     0.0  0.0     0.00       0.0     0.00  0.0  0.0    0.0   \n",
       "892   ...     0.0  0.0     0.00       0.0     0.00  0.0  0.0    0.0   \n",
       "2709  ...     0.0  0.0     0.56       0.0     0.56  0.0  0.0    0.0   \n",
       "\n",
       "      conference  target  \n",
       "3938        0.00       0  \n",
       "3533        0.00       0  \n",
       "3110        2.56       0  \n",
       "892         0.00       1  \n",
       "2709        0.00       0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word, we now tally up the number of spam and non-spam messages that word was in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_spams_hams(train):\n",
    "    \"\"\"\n",
    "    Counts the number of messages that contain at least one\n",
    "    instence of a particular word (non-zero probability). \n",
    "    \"\"\"\n",
    "    spam_counts = collections.defaultdict(int)\n",
    "    ham_counts = collections.defaultdict(int)\n",
    "\n",
    "    # loop over the messages in the training set\n",
    "    for _, message in train.iterrows():\n",
    "        spam_status = message['target']\n",
    "\n",
    "        # Loop over all word_categories and check if any of them were \n",
    "        # in a spam or non-spam message (had a non-zero probability)\n",
    "        for i, word in enumerate(word_catagories):\n",
    "            # if spam and word is in the message\n",
    "            if (message[i] > 0) and (spam_status): \n",
    "                # Add the number of times word occured in that message\n",
    "                spam_counts[word] += 1\n",
    "            # if non-spam message contains word\n",
    "            elif (message[i] > 0) and (not spam_status): \n",
    "                ham_counts[word] += 1\n",
    "    return spam_counts, ham_counts\n",
    "\n",
    "spam_counts, ham_counts = count_spams_hams(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the likelihoods that a message is spam or not spam given a word (train the Naive Bayes classifier). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_probabilities(train, spam_counts, ham_counts, k=1):\n",
    "    \"\"\"\n",
    "    Calculates the likelihood of word given spam and word given not spam.\n",
    "    Returns a tuple of (word, p(word|spam), p(word|!spam)). k is the \n",
    "    smoothing factor\n",
    "    \"\"\"\n",
    "    n_spams = sum(train.target) # total number of spam messages \n",
    "    n_hams = train.shape[0] - n_spams\n",
    "    \n",
    "    t = [(word, \n",
    "         (k + spam_counts[word])/(2*k + n_spams),\n",
    "         (k + ham_counts[word])/(2*k + n_hams)\n",
    "         ) for word in spam_counts.keys()]\n",
    "    return t\n",
    "\n",
    "word_likelihood = word_probabilities(train, spam_counts, ham_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in the trained likelihoods and calculates the probability that the message is spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spam_probability(word_likelihoods, message):\n",
    "    \"\"\"\n",
    "    Calculate the probability that the message is spam, given the\n",
    "    word likelihoods.    \n",
    "    \"\"\"\n",
    "    log_prob_spam = log_prob_ham = 0.0\n",
    "    \n",
    "    for word, spam_like, ham_like in word_likelihoods:\n",
    "        # Find the index of the probability in message that\n",
    "        # corresponds to the likelihood we are currently comparing\n",
    "        idx = np.where(word == word_catagories)[0]\n",
    "        assert len(idx) == 1, f\"One or too many matches! idx={idx}\"\n",
    "        \n",
    "        # If word was found in the message.\n",
    "        if message[idx[0]] > 0:\n",
    "            log_prob_spam += np.log(spam_like)\n",
    "            log_prob_ham += np.log(ham_like)\n",
    "        # If the word was not found, add the probability of not seeing it.\n",
    "        else:\n",
    "            log_prob_spam += np.log(1-spam_like)\n",
    "            log_prob_ham += np.log(1-ham_like)\n",
    "            \n",
    "    prob_spam = np.exp(log_prob_spam)\n",
    "    prob_ham = np.exp(log_prob_ham)\n",
    "    return prob_spam / (prob_spam + prob_ham)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate the classifier with the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_targets = np.zeros(validate.shape[0])\n",
    "\n",
    "for i, (_, row) in enumerate(validate.iterrows()):\n",
    "    p_i = spam_probability(word_likelihood, row)\n",
    "    if p_i > 0.5:\n",
    "        filter_targets[i] = 1\n",
    "    else:\n",
    "        filter_targets[i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the confusion matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1270  125]\n",
      " [ 162  744]] =\n",
      "\n",
      " [['TP' 'FP']\n",
      " ['FN' 'TN']] \n",
      " FP = type 1 error | FN = type 2 error \n",
      "(rows: spedicted spam and not spam. columns: actual spam and actual ham)\n"
     ]
    }
   ],
   "source": [
    "confusion = sklearn.metrics.confusion_matrix(validate.target, filter_targets)\n",
    "print(confusion, '=\\n\\n', np.array([['TP', 'FP'], ['FN', 'TN']]), \n",
    "      '\\n FP = type 1 error | FN = type 2 error',\n",
    "      '\\n(rows: spedicted spam and not spam. columns: actual spam and actual ham)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.0 % of messages were correctly classified as spam or not spam\n"
     ]
    }
   ],
   "source": [
    "print(round(100*(confusion[0, 0] + confusion[1, 1])/np.sum(confusion)), '% of messages were correctly classified as spam or not spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
