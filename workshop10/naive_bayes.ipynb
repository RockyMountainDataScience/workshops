{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Spam Filter\n",
    "In this notebook we will lean about, train and validate a Naive Bayes email spam filter (classifier). The plan is to develop the classifier and then train and validate it on the [UCI spambase dataset](https://archive.ics.uci.edu/ml/datasets/Spambase) to determine what messages are spam or not spam. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem\n",
    "\n",
    "Given the event that the message is spam by S and the message contains a word by W, Bayes theorem allows us to calculate the probability that the message is spam given the word(s) from the trained probabilities of the message is spam if it contains a word. \n",
    "\n",
    "$$ \n",
    "P(S|W) = \\frac{P(W|S)P(S)}{P(W)} = \\frac{P(W|S)P(S)}{P(W|S)P(S) + P(W|!S)P(!S)} \n",
    "$$\n",
    "\n",
    "*Example*: Assume that spam and non spam messages are equally likely - P(S) = P(!S) = 0.5. Then also assume tha the word 'viagra' appears in 50% of spam emails and 1% in non-spam emails. Then Bayes theorem implies that the probability that a given email containing viagra is spam is\n",
    "\n",
    "$$\n",
    "P(S|viagra) = \\frac{P(viagra|S)}{P(viagra|S) + P(viagra|!S)} = \\frac{0.5}{0.5 + 0.01} = 98 \\%.\n",
    "$$\n",
    "\n",
    "This ML algorithm is naive because the probabiltities of words being in spam or ham messages are independent.\n",
    "\n",
    "The goal of this alogirthm is to calculate P(W|S) and P(W|!S) and we will do this in the following steps\n",
    "- Load the data. This UCI dataset contains 48 columns with occurance frequenices of 48 different words and the last column is the target (spam or ham). Use the load_data() function\n",
    "- Split the data into a training and validation subsets Use the split_data() function\n",
    "- For each of the 48 words, calculate the number of spam and ham emails that word is found in. Use the count_spams_hams() function\n",
    "- Calculate the liklihood that a message is spam or ham for each word using the word_probabilities() function\n",
    "- Write a function that takes in the likelihoods and a message and uses Bayes theorem to calculate P(S|W) (where W is a set of words in the message) and classifies the message as spam or ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import sklearn.metrics\n",
    "import sklearn.naive_bayes\n",
    "\n",
    "import get_words\n",
    "\n",
    "np.random.seed(123) # So you can exactly reproduce my numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_catagories = np.array(get_words.get_words())\n",
    "\n",
    "def load_data():\n",
    "    \"\"\" \n",
    "    Load the spambase dataset\n",
    "    \"\"\"\n",
    "    column_names = np.concatenate((word_catagories, ['target']))\n",
    "    column_numbers = np.concatenate((np.arange(len(word_catagories)), [57]))\n",
    "\n",
    "    data = pd.read_csv(os.path.join('.', 'data', 'spambase.data'), \n",
    "                       names=column_names, usecols=column_numbers)\n",
    "    #data[word_catagories] *= 10000 # Scaling factor to go from percentage to number of words \n",
    "    return data\n",
    "\n",
    "def split_data(data, p):\n",
    "    \"\"\"\n",
    "    Split the data according to the probability p of picking a training sample.\n",
    "    Returns data with the same number of columns, except the first DataFrame \n",
    "    (training set) has p*data.shape[0] number of rows and the second DataFrame \n",
    "    (validation set) has (1-p)*data.shape[0] number of rows.\n",
    "    \"\"\" \n",
    "    n_samples = data.shape[0]\n",
    "    n_train = int(n_samples * p)\n",
    "    idt = np.random.choice(np.arange(n_samples), size=n_train, replace=False)\n",
    "    idv = np.array([i for i in range(n_samples) if i not in idt])\n",
    "    return data.loc[idt], data.loc[idv]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the spambase dataset, and split it into a train and validate subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()\n",
    "train, validate = split_data(data, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>make</th>\n",
       "      <th>address</th>\n",
       "      <th>all</th>\n",
       "      <th>our</th>\n",
       "      <th>over</th>\n",
       "      <th>remove</th>\n",
       "      <th>internet</th>\n",
       "      <th>order</th>\n",
       "      <th>mail</th>\n",
       "      <th>receive</th>\n",
       "      <th>...</th>\n",
       "      <th>direct</th>\n",
       "      <th>cs</th>\n",
       "      <th>meeting</th>\n",
       "      <th>original</th>\n",
       "      <th>project</th>\n",
       "      <th>re</th>\n",
       "      <th>edu</th>\n",
       "      <th>table</th>\n",
       "      <th>conference</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2786</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3269</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1.32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3118</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      make  address   all  our  over  remove  internet  order  mail  receive  \\\n",
       "2786   0.0      0.0  0.00  0.0  0.00     0.0       0.0   0.00  0.00     0.00   \n",
       "463    0.0      0.0  0.00  0.0  0.00     0.0       0.0   1.08  0.00     0.00   \n",
       "3269   0.0      0.0  0.00  0.0  0.00     0.0       0.0   0.00  0.00     0.00   \n",
       "380    0.6      0.0  0.36  0.0  1.44     0.0       0.0   0.00  0.24     1.32   \n",
       "3118   0.0      0.0  0.00  0.0  0.00     0.0       0.0   0.00  0.00     0.00   \n",
       "\n",
       "      ...  direct   cs  meeting  original  project   re  edu  table  \\\n",
       "2786  ...     0.0  0.0      0.0       0.0     0.00  0.0  0.0    0.0   \n",
       "463   ...     0.0  0.0      0.0       0.0     0.00  0.0  0.0    0.0   \n",
       "3269  ...     0.0  0.0      0.0       0.0     0.89  0.0  0.0    0.0   \n",
       "380   ...     0.0  0.0      0.0       0.0     0.00  0.0  0.0    0.0   \n",
       "3118  ...     0.0  0.0      0.0       0.0     0.00  0.0  0.0    0.0   \n",
       "\n",
       "      conference  target  \n",
       "2786        0.00       0  \n",
       "463         0.00       1  \n",
       "3269        0.89       0  \n",
       "380         0.00       1  \n",
       "3118        0.00       0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word, we now tally up the number of spam and non-spam messages that word was in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_spams_hams(train):\n",
    "    \"\"\"\n",
    "    Counts the number of messages that contain at least one\n",
    "    instence (non-zero probability) of each word. \n",
    "    \"\"\"\n",
    "    # key is the word and value is an integer value of how many\n",
    "    # spam or ham messages this word was in\n",
    "    spam_counts = collections.defaultdict(int)\n",
    "    ham_counts = collections.defaultdict(int)\n",
    "\n",
    "    # loop over the messages in the training set\n",
    "    for _, message in train.iterrows():\n",
    "        spam_status = message['target']\n",
    "\n",
    "        # Loop over all word_categories and check if any of them were \n",
    "        # in a spam or non-spam message (had a non-zero probability)\n",
    "        for i, word in enumerate(word_catagories):\n",
    "            # if spam and word is in the message\n",
    "            if (message[i] > 0) and (spam_status): \n",
    "                # Add the number of times word occured in that message\n",
    "                spam_counts[word] += 1\n",
    "            # if non-spam message contains word\n",
    "            elif (message[i] > 0) and (not spam_status): \n",
    "                ham_counts[word] += 1\n",
    "    return spam_counts, ham_counts\n",
    "\n",
    "spam_counts, ham_counts = count_spams_hams(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the likelihoods that a message is spam or not spam given each word (train the Naive Bayes classifier). \n",
    "\n",
    "P(w|S) = (k + numer of spams containing word w)\\(2k + numer of spams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_probabilities(train, spam_counts, ham_counts, k=1):\n",
    "    \"\"\"\n",
    "    Calculates the likelihood of word given spam and word given not spam.\n",
    "    Returns a tuple of (word, p(word|spam), p(word|!spam)). k is the \n",
    "    smoothing factor\n",
    "    \"\"\"\n",
    "    n_spams = sum(train.target) # total number of spam messages \n",
    "    n_hams = train.shape[0] - n_spams\n",
    "    \n",
    "    t = [(word, \n",
    "         (k + spam_counts[word])/(k + n_spams),\n",
    "         (k + ham_counts[word])/(k + n_hams)\n",
    "         ) for word in spam_counts.keys()]\n",
    "    return t\n",
    "\n",
    "word_likelihood = word_probabilities(train, spam_counts, ham_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in the trained word_likelihoods and calculates the probability that the message is spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spam_probability(word_likelihoods, message):\n",
    "    \"\"\"\n",
    "    Calculate the probability that the message is spam, given the\n",
    "    word likelihoods. We add the log of the probabilities to avoid\n",
    "    a numerical underflow error.\n",
    "    \"\"\"\n",
    "    log_prob_spam = log_prob_ham = 0.0\n",
    "    \n",
    "    for word, spam_like, ham_like in word_likelihoods:\n",
    "        # Find the index of the probability in message that\n",
    "        # corresponds to the likelihood we are currently comparing\n",
    "        idx = np.where(word == word_catagories)[0]\n",
    "        assert len(idx) == 1, f\"One or too many matches! idx={idx}\"\n",
    "        \n",
    "        # If word was found in the message.\n",
    "        if message[idx[0]] > 0:\n",
    "            log_prob_spam += np.log(spam_like)\n",
    "            log_prob_ham += np.log(ham_like)\n",
    "        # If the word was not found, add the probability of not seeing it.\n",
    "        else:\n",
    "            log_prob_spam += np.log(1-spam_like)\n",
    "            log_prob_ham += np.log(1-ham_like)\n",
    "            \n",
    "    prob_spam = np.exp(log_prob_spam)\n",
    "    prob_ham = np.exp(log_prob_ham)\n",
    "    return prob_spam / (prob_spam + prob_ham)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate the classifier using the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_targets = np.zeros(validate.shape[0])\n",
    "\n",
    "for i, (_, row) in enumerate(validate.iterrows()):\n",
    "    p_i = spam_probability(word_likelihood, row)\n",
    "    if p_i > 0.5:\n",
    "        filter_targets[i] = 1\n",
    "    else:\n",
    "        filter_targets[i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the confusion matrix for this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1260  140]\n",
      " [ 176  725]] =\n",
      "\n",
      " [['TP' 'FP']\n",
      " ['FN' 'TN']] \n",
      " FP = type 1 error | FN = type 2 error \n",
      "(rows: pedicted spam and ham. columns: actual spam and ham)\n"
     ]
    }
   ],
   "source": [
    "confusion = sklearn.metrics.confusion_matrix(validate.target, filter_targets)\n",
    "print(confusion, '=\\n\\n', np.array([['TP', 'FP'], ['FN', 'TN']]), \n",
    "      '\\n FP = type 1 error | FN = type 2 error',\n",
    "      '\\n(rows: pedicted spam and ham. columns: actual spam and ham)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.0 % of messages were correctly classified as spam or not spam\n"
     ]
    }
   ],
   "source": [
    "print(round(100*(confusion[0, 0] + confusion[1, 1])/np.sum(confusion)), \n",
    "      '% of messages were correctly classified as spam or not spam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad for such a \"naive\" classifier! We can do better with more data (spambase.data has other columns that I did not use here), tweak the smoothing (k) factor, and add a fraction threshold to consider that word in a spam or ham message. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets find the spammiest and hammiest words - P(S|W)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spammiest words: [('money', 0.05585980284775466, 0.005039596832253419), ('hp', 0.3362541073384447, 0.024478041756659467), ('your', 0.20262869660460023, 0.014398848092152628), ('hpl', 0.38116100766703176, 0.014398848092152628), ('internet', 0.411829134720701, 0.012239020878329733)]\n",
      "Hammiest words: [('labs', 0.007667031763417305, 0.28005759539236863), ('pm', 0.002190580503833516, 0.07919366450683946), ('conference', 0.002190580503833516, 0.051115910727141826), ('lab', 0.013143483023001095, 0.29301655867530596), ('george', 0.019715224534501644, 0.3779697624190065)]\n"
     ]
    }
   ],
   "source": [
    "def p_spam_given_word(word_like):\n",
    "    \"\"\"\n",
    "    Calculate P(S|W) using Bayes theorem for all words in the likelihoods tuples. \n",
    "    \"\"\"\n",
    "    word, p_if_spam, p_if_ham = word_like\n",
    "    p = p_if_spam/(p_if_spam + p_if_ham)\n",
    "    return p\n",
    "\n",
    "word_p = sorted(word_likelihood, key=p_spam_given_word)\n",
    "print(f'Spammiest words: {word_p[-5:]}')\n",
    "print(f'Hammiest words: {word_p[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks: \n",
    "- Use a library (e.g. scikit-learn) to classify and validate this dataset. *Hint* use sklearn.naive_bayes.BernoulliNB()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
