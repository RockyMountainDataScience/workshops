{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees for Classification\n",
    "\n",
    "- In this workshop, we will learn how to conduct the [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning) task of [classification](https://en.wikipedia.org/wiki/Statistical_classification) (as opposed to [regression](https://en.wikipedia.org/wiki/Regression_analysis)) using a [decision tree](https://en.wikipedia.org/wiki/Decision_tree_learning) model and how to tune the model using [cross-validation (CV)](https://en.wikipedia.org/wiki/Cross-validation_(statistics)). If you want to look at an in depth discussion of these concepts, take a look at this great [book](https://web.stanford.edu/~hastie/ElemStatLearn/) by Hastie, Tibshirani and Friedman.\n",
    "\n",
    "After this workshop, you should know how to\n",
    "\n",
    "- Model data using a decision tree\n",
    "- Tune the decision tree to generalize to unseen data using cross-validation\n",
    "\n",
    "We will use the famous [iris](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/iris.html) dataset gathered by Edgar Anderson to illustrate this process. The iris dataset is famous as it was [used](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x) by famous statistician R.A. Fisher to illustrate classification of iris species using [Linear Discriminant Analysis (LDA)](https://en.wikipedia.org/wiki/Linear_discriminant_analysis). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Packages\n",
    "from sklearn.datasets import load_iris               # Function to load iris data\n",
    "from sklearn.tree import DecisionTreeClassifier      # Build decision tree\n",
    "from sklearn.tree.export import export_text          # print text tree\n",
    "from sklearn import tree                             # to plot tree\n",
    "from sklearn.model_selection import train_test_split # Split data into train and test\n",
    "\n",
    "import matplotlib.pyplot as plt                      # For plotting data\n",
    "import matplotlib.patches as mpatches                # For building handles for legends\n",
    "\n",
    "import numpy as np                                   # General numeric library\n",
    "\n",
    "# Import Data\n",
    "iris = load_iris()   # Load Iris data\n",
    "X = iris['data']     # Extract variables\n",
    "y = iris['target']   # Extract response\n",
    "desc = iris['DESCR'] # Extract description of data\n",
    "\n",
    "print(desc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset Data for two\n",
    "X_s = X[:,2:4]\n",
    "\n",
    "# Extract arrays of each variable\n",
    "sepal_length = X[:,0:1].reshape((150,))\n",
    "sepal_width = X[:,1:2].reshape((150,))\n",
    "petal_width = X[:,2:3].reshape((150,))\n",
    "petal_length = X[:,3:4].reshape((150,))\n",
    "\n",
    "\n",
    "# Let's look at only two variables, petal width and petal length\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(petal_width, petal_length, c=y, s=4)\n",
    "plt.xlabel(\"Petal Width\")\n",
    "plt.ylabel(\"Petal Height\")\n",
    "setosa = mpatches.Patch(color='purple', label='Setosa')\n",
    "versicolor = mpatches.Patch(color='green', label='Versicolor')\n",
    "virginica = mpatches.Patch(color='yellow', label='Virginica')\n",
    "plt.legend(handles=[setosa, versicolor, virginica])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a simple decision tree classifier with a maximum depth of 2 to predict iris species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Classifier\n",
    "clf = tree.DecisionTreeClassifier(random_state = 42, max_depth=2)\n",
    "clf = clf.fit(X_s, y)\n",
    "tree.plot_tree(clf)\n",
    "print(\"Dendrogram for Decision Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the data with these decision boundaries\n",
    "plt.scatter(petal_width, petal_length, c=y, s=4)\n",
    "plt.legend(handles=[setosa, versicolor, virginica])\n",
    "plt.axvline(x=2.45)\n",
    "plt.axhline(y=1.75)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick note on how optimal decision boundaries are being calculated:\n",
    "\n",
    "For classification, one metric to evaluate different splits is mean decrease in [gini impurity](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity). It effectively estimates how expected classification accuracy would change with a given split.\n",
    "\n",
    "For the regression case, we can simply use [reduction in mean squared error](https://en.wikipedia.org/wiki/Decision_tree_learning#Variance_reduction) as a metric for building trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate performance of this model on this dataset. First, let's \n",
    "predict some new data points with this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's predict a few query points using the model\n",
    "p1 = clf.predict([[1, 1]])\n",
    "p2 = clf.predict([[3, 1]])\n",
    "p3 = clf.predict([[4, 2]])\n",
    "\n",
    "print(f'Point (1, 1) is predicted to be {iris[\"target_names\"][int(p1)]}',\n",
    "      f'Point (3, 1) is predicted to be {iris[\"target_names\"][int(p2)]}',\n",
    "      f'Point (4, 2) is predicted to be {iris[\"target_names\"][int(p3)]}', \n",
    "      sep = '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy of the model on the data\n",
    "sum(clf.predict(X_s) == y) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. This simple model seems to fit fairly well. However, we only allowed a maximum of two splits in our decision tree. What happens if we allow more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Classifier\n",
    "clf = tree.DecisionTreeClassifier(random_state = 42, max_depth=20)\n",
    "clf = clf.fit(X_s, y)\n",
    "tree.plot_tree(clf)\n",
    "sum(clf.predict(X_s) == y) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. This simple model seems to fit very well. However, a question one might ask is\n",
    "what will be the classification accuracy on an unseen (new) dataset? For example, if another researcher came to you with a different dataset, how well will your model perform on their data?\n",
    "\n",
    "Note that some of the terminal nodes have few samples. We might expect predictions on these nodes to not estimate probability of a given class with very much accuracy. This illustrates the problem of overfitting.\n",
    "\n",
    "To estimate the accuracy on unseen data, we use a technique\n",
    "called **cross-validation**. We simply repeat the process of randomly splitting our dataset into a training set to train the model on and test set to evaluate the model on and average a performance metric (accuracy in this setting) across replications. First let's look at one iteration of this process. Consider the following code and the image generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, \n",
    "                                                        test_size=0.4, random_state = 42)\n",
    "# Extract just the two variables we're interested in\n",
    "x_train_s = x_train[:, 2:4]\n",
    "x_test_s = x_test[:, 2:4]\n",
    "clf = tree.DecisionTreeClassifier(random_state = 7, max_depth=2)\n",
    "clf = clf.fit(x_train_s, y_train)\n",
    "\n",
    "# Plot the decision tree\n",
    "tree.plot_tree(clf)\n",
    "\n",
    "# Plot the training set and testing set with these decision boundaries\n",
    "pred_train = clf.predict(x_test_s)\n",
    "pred_train\n",
    "x_train_s[:,1]\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "ax1.scatter(x_train_s[:,0], x_train_s[:,1], c=y_train, s=4)\n",
    "ax1.set_title(\"Training Set\")\n",
    "ax1.axhline(y=0.8)\n",
    "ax1.axhline(y=1.75)\n",
    "\n",
    "\n",
    "ax2.scatter(x_test_s[:,0], x_test_s[:,1], c=pred_train, s=4)\n",
    "ax2.set_title(\"Testing Set\")\n",
    "ax2.axhline(y=0.8)\n",
    "ax2.axhline(y=1.75)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we repeat this process 500 times to estimate the mean predictive performance of a decision tree with a max_depth equal to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 500\n",
    "acc = []\n",
    "for i in range(reps):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, \n",
    "                                                        test_size=0.4)\n",
    "    x_train_s = x_train[:, 2:4]\n",
    "    x_test_s = x_test[:, 2:4]\n",
    "    clf = tree.DecisionTreeClassifier(random_state = 42, max_depth=20)\n",
    "    clf = clf.fit(x_train_s, y_train)\n",
    "    acc.append(sum(clf.predict(x_test_s) == y_test) / len(y_test))\n",
    "\n",
    "np.mean(acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are other (better ways) to conduct CV, and `sklearn` has a a number of modules for conducting this task (e.g. `sklearn.model_selection.cross_validate`. For now, we will leave exploring these other methods up to you. \n",
    "\n",
    "Now, notice how much lower performance the cross-validation error is to the model fit with the entire dataset. This suggests the model fit on the entire dataset is overfitting the data, as it doesn't match the performance we expect to see on a new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use cross-validation to tune the depth of our tree by estimating cross-validation accuracy across a range of depths and choosing the depth whose estimated cross-validation accuracy is largest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reps = 500\n",
    "depths = [x+1 for x in range(20)]\n",
    "acc = []\n",
    "for j in depths:\n",
    "    accd = []\n",
    "    for i in range(reps):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, \n",
    "                                                            test_size=0.4)\n",
    "        x_train_s = x_train[:, 2:4]\n",
    "        x_test_s = x_test[:, 2:4]\n",
    "        clf = tree.DecisionTreeClassifier(random_state = 42, max_depth=j)\n",
    "        clf = clf.fit(x_train_s, y_train)\n",
    "        accd.append(sum(clf.predict(x_test_s) == y_test) / len(y_test))\n",
    "    acc.append(accd)\n",
    "\n",
    "errs = np.apply_along_axis(np.mean, 1, acc)\n",
    "\n",
    "errs = np.asarray(errs)\n",
    "depths = np.asarray(depths)\n",
    "\n",
    "plt.scatter(depths, errs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "er = list(errs)\n",
    "\n",
    "print(f'Optimal max tree-depth: {depths[er.index(max(er))]}',\n",
    "      f'Optimal Average Cross validated accuracy: {max(er)}',\n",
    "      sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like a depth of {{depths[er.index(max(er))]}} is optimal for our model to generalize to unseen data and has an average cross-validated accuracy of about {{round(100* max(er), 2)}}%. Let's now fit the full model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick note about the above markdown chunk: It evaluates python within the markdown chunk. But this is a feature [not yet](https://github.com/jupyter/notebook/issues/1915) built into jupyter, but can be enabled by installing the [jupyter_contrib_nbextensions](https://github.com/ipython-contrib/jupyter_contrib_nbextensions) package and enabling the appropriate [extension](https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions.html) (python-markdown). You can do this with the following steps:\n",
    "\n",
    "1. `pip install jupyter_contrib_nbextensions`\n",
    "2. `jupyter contrib nbextension install --user`\n",
    "3. `jupyter nbextension enable python-markdown/main`\n",
    "\n",
    "or replacing step 1 with:\n",
    "`conda install -c conda-forge jupyter_contrib_nbextensions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Classifier\n",
    "clf = tree.DecisionTreeClassifier(random_state = 42, max_depth=depths[er.index(max(er))])\n",
    "clf = clf.fit(X_s, y)\n",
    "tree.plot_tree(clf)\n",
    "sum(clf.predict(X_s) == y) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we get a higher accuracy on our dataset. However, this overestimates the accuracy on unseen data that is estimated via cross-validation.\n",
    "\n",
    "Now, we only used two of the four variables in our dataset to build our model. Let's build and tune the model using the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 500 #CV Reps\n",
    "depths = [x+1 for x in range(20)] # Range of tuning parameter (depth)\n",
    "acc = []\n",
    "for j in depths:\n",
    "    accd = []\n",
    "    for i in range(reps):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, \n",
    "                                                            test_size=0.4)\n",
    "        clf = tree.DecisionTreeClassifier(random_state = 42, max_depth=j)\n",
    "        clf = clf.fit(x_train, y_train)\n",
    "        accd.append(sum(clf.predict(x_test) == y_test) / len(y_test))\n",
    "    acc.append(accd)\n",
    "\n",
    "errs = np.apply_along_axis(np.mean, 1, acc)\n",
    "\n",
    "errs = np.asarray(errs)\n",
    "depths = np.asarray(depths)\n",
    "\n",
    "plt.scatter(depths, errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "er = list(errs)\n",
    "\n",
    "print(f'Optimal tree depth: {depths[er.index(max(er))]}',\n",
    "      f'Optimal Average Cross validated accuracy: {max(er)}',\n",
    "      sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the extra variables gave us very little extra information to improve our classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "- Use the code above to build and tune a classifier for the wine dataset (see code below)\n",
    "- Use the code above to build and tune a regressor for the boston dataset (hint: you may want to use the [DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) class object from scikit-learn )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Load Data\n",
    "wine = load_wine()\n",
    "X = wine['data']\n",
    "y = wine['target']\n",
    "desc = wine['DESCR']\n",
    "\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Load Data\n",
    "boston = load_boston()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
